{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intership Project Assignment of Blackcoffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input file and convert column 'SECFNAME' into Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.ExcelFile(r\"C:\\Users\\YASHA\\cik_list.xlsx\")\n",
    "df = df.parse('cik_list_ajay')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "1    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "2    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "3    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "4    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "Name: SECFNAME, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the initial structure of the link in secfname\n",
    "link = 'https://www.sec.gov/Archives/'\n",
    "df.SECFNAME = link+df.SECFNAME\n",
    "df.SECFNAME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO       FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803  1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805  1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808  1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811  1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811  1998-11-16  NT 10-Q   \n",
       "\n",
       "                                            SECFNAME  \n",
       "0  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "1  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "2  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "3  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "4  https://www.sec.gov/Archives/edgar/data/3662/0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(r'C:\\Users\\YASHA\\cik_list1.csv',index=False) \n",
    "df = pd.read_csv(r\"C:\\Users\\YASHA\\cik_list1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create required Docmunents files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = xlrd.open_workbook('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "sheet = book.sheet_by_name('LoughranMcDonald_MasterDictiona')\n",
    "# read header values into the list\n",
    "\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "# print(sheet.ncols)\n",
    "# print(keys)\n",
    "\n",
    "neg_list = []\n",
    "pos_list = []\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    if sheet.cell(row_index, 7).value > 0:\n",
    "        d = sheet.cell(row_index, 0).value\n",
    "        neg_list.append(d)\n",
    "    if sheet.cell(row_index, 8).value > 0:\n",
    "        d = sheet.cell(row_index, 0).value\n",
    "        pos_list.append(d)\n",
    "        \n",
    "with open(\"PositiveWords.txt\", \"w+\") as f:\n",
    "    for i in pos_list:\n",
    "        f.write(str(i)+ \"\\n\")\n",
    "\n",
    "with open(\"NegativeWords.txt\", \"w+\") as g:\n",
    "    for j in neg_list:\n",
    "        g.write(str(j)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = xlrd.open_workbook(r'C:\\Users\\YASHA\\Blackcoffer\\uncertainty_dictionary.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "# read header values into the list\n",
    "\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "# print(sheet.ncols)\n",
    "# print(keys)\n",
    "\n",
    "uc_list = []\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "        uc_list.append(sheet.cell(row_index, 0).value)\n",
    "        \n",
    "with open(\"uncertainty_dictionary.txt\", \"w+\") as f:\n",
    "    for i in uc_list:\n",
    "        f.write(str(i)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = xlrd.open_workbook(r'C:\\Users\\YASHA\\Blackcoffer\\constraining_dictionary.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "# read header values into the list\n",
    "\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "# print(sheet.ncols)\n",
    "\n",
    "con_list = []  \n",
    "\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "        con_list.append(sheet.cell(row_index, 0).value)\n",
    "        \n",
    "with open(\"constraining_dictionary.txt\", \"w+\") as f:\n",
    "    for i in con_list:\n",
    "        f.write(str(i)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction patterns\n",
    "mda_regex = r\"item[^a-zA-Z\\n]*\\d\\s*\\.\\s*management\\'s discussion and analysis.*?^\\s*item[^a-zA-Z\\n]*\\d\\s*\\.*\"\n",
    "qqd_regex = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Quantitative and Qualitative Disclosures about \"             r\"Market Risk.*?^\\s*item\\s*\\d\\s*\"\n",
    "riskfactor_regex = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Risk Factors.*?^\\s*item\\s*\\d\\s*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath locations\n",
    "stopWordsFile = r'C:\\Users\\YASHA\\StopWords_Generic.txt'\n",
    "positiveWordsFile = r'C:\\Users\\YASHA\\PositiveWords.txt'\n",
    "nagitiveWordsFile = r'C:\\Users\\YASHA\\NegativeWords.txt'\n",
    "uncertainty_dictionaryFile = r'C:\\Users\\YASHA\\uncertainty_dictionary.txt'\n",
    "constraining_dictionaryFile = r'C:\\Users\\YASHA\\constraining_dictionary.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting requried text\n",
    "def rawdata_extract(path, cikListFile):\n",
    "    html_regex = re.compile('<.*?>')\n",
    "    extraxted_data=[]\n",
    "    cikListFile = pd.read_csv(cikListFile)\n",
    "    for index, row in cikListFile.iterrows():\n",
    "        processingFile=row['SECFNAME'].split('/')\n",
    "        inputFile = processingFile[3]\n",
    "        cik=row['CIK']\n",
    "        coname=row['CONAME']\n",
    "        fyrmo=row['FYRMO']\n",
    "        fdate = row['FDATE']\n",
    "        form = row['FORM']\n",
    "        secfname=row['SECFNAME']\n",
    "        for fileName in os.listdir(path):\n",
    "            filenameopen = os.path.join(path, fileName)\n",
    "            dirFileName = filenameopen.split('\\\\')\n",
    "            currentFile=dirFileName[1]\n",
    "\n",
    "            if os.path.isfile(filenameopen) and currentFile == inputFile :\n",
    "                resultdict = dict()\n",
    "                resultdict['CIK'] = cik\n",
    "                resultdict['CONAME'] = coname\n",
    "                resultdict['FYRMO'] = fyrmo\n",
    "                resultdict['FDATE'] = fdate\n",
    "                resultdict['FORM'] = form\n",
    "                resultdict['SECFNAME'] = secfname\n",
    "                \n",
    "                \n",
    "                with open(filenameopen, 'r', encoding='utf-8', errors=\"replace\") as in_file:\n",
    "                    content = in_file.read()\n",
    "                    content = re.sub(html_regex,'',content)\n",
    "                    content = content.replace('&nbsp;','')\n",
    "                    content = re.sub(r'&#\\d+;', '', content)\n",
    "                    matches_mda = re.findall(mda_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if matches_mda:\n",
    "                        result = max(matches_mda, key=len)\n",
    "                        result = str(result).replace('\\n', '')\n",
    "                        resultdict['mda_extract'] = result\n",
    "                    else:\n",
    "                        resultdict['mda_extract'] = \"\"\n",
    "                    match_qqd = re.findall(qqd_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if match_qqd:\n",
    "                        result_qqd = max(match_qqd, key=len)\n",
    "                        result_qqd = str(result_qqd).replace('\\n','')\n",
    "                        resultdict['qqd_extract']= result_qqd\n",
    "                    else:\n",
    "                        resultdict['qqd_extract'] = \"\"\n",
    "                    match_riskfactor = re.findall(riskfactor_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if match_riskfactor:\n",
    "                        result_riskfactor = max(match_riskfactor, key=len)\n",
    "                        result_riskfactor = str(result_riskfactor).replace('\\n', '')\n",
    "                        resultdict['riskfactor_extract'] = result_riskfactor\n",
    "                    else:\n",
    "                        resultdict['riskfactor_extract'] = \"\"\n",
    "                    extraxted_data.append(resultdict)\n",
    "\n",
    "                in_file.close()\n",
    "\n",
    "    return extraxted_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 1.1: Positive score, negative score, polarity score\n",
    "\n",
    "# Loading stop words dictionary for removing stop words\n",
    "\n",
    "\n",
    "with open(stopWordsFile ,'r') as stop_words:\n",
    "    stopWords = stop_words.read().lower()\n",
    "stopWordList = stopWords.split('\\n')\n",
    "stopWordList[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading positive words\n",
    "with open(positiveWordsFile,'r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading negative words\n",
    "with open(nagitiveWordsFile ,'r') as negfile:\n",
    "    negativeword=negfile.read().lower()\n",
    "negativeWordList=negativeword.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating positive score \n",
    "def positive_score(text):\n",
    "    numPosWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in positiveWordList:\n",
    "            numPosWords  += 1\n",
    "    \n",
    "    sumPos = numPosWords\n",
    "    return sumPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Negative score\n",
    "def negative_word(text):\n",
    "    numNegWords=0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in negativeWordList:\n",
    "            numNegWords -=1\n",
    "    sumNeg = numNegWords \n",
    "    sumNeg = sumNeg * -1\n",
    "    return sumNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating polarity score\n",
    "def polarity_score(positiveScore, negativeScore):\n",
    "    pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "    return pol_score\n",
    "\n",
    "\n",
    "# # Section 2 -Analysis of Readability -  Average Sentence Length, percentage of complex words, fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Average sentence length \n",
    "# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n",
    "     \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = tokenizer(text)\n",
    "    totalWordCount = len(tokens)\n",
    "    totalSentences = len(sentence_list)\n",
    "    average_sent = 0\n",
    "    if totalSentences != 0:\n",
    "        average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "    average_sent_length= average_sent\n",
    "    \n",
    "    return round(average_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of complex word \n",
    "# It is calculated using Percentage of Complex words = the number of complex words / the number of words \n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Fog Index \n",
    "# Fog index is calculated using -- Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 4: Complex word count\n",
    "# Counting complex words\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 5: Word count\n",
    "#Counting total words\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# calculating uncertainty_score\n",
    "with open(uncertainty_dictionaryFile ,'r') as uncertain_dict:\n",
    "    uncertainDict=uncertain_dict.read().lower()\n",
    "uncertainDictionary = uncertainDict.split('\\n')\n",
    "\n",
    "def uncertainty_score(text):\n",
    "    uncertainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in uncertainDictionary:\n",
    "            uncertainWordnum +=1\n",
    "    sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "    return sumUncertainityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating constraining score\n",
    "with open(constraining_dictionaryFile ,'r') as constraining_dict:\n",
    "    constrainDict=constraining_dict.read().lower()\n",
    "constrainDictionary = constrainDict.split('\\n')\n",
    "\n",
    "def constraining_score(text):\n",
    "    constrainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnum +=1\n",
    "    sumConstrainScore = constrainWordnum \n",
    "    \n",
    "    return sumConstrainScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating positive word proportion\n",
    "\n",
    "def positive_word_prop(positiveScore,wordcount):\n",
    "    positive_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "    return positive_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating negative word proportion\n",
    "\n",
    "def negative_word_prop(negativeScore,wordcount):\n",
    "    negative_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "    return negative_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating uncertain word proportion\n",
    "\n",
    "def uncertain_word_prop(uncertainScore,wordcount):\n",
    "    uncertain_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "    return uncertain_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating constraining word proportion\n",
    "\n",
    "def constraining_word_prop(constrainingScore,wordcount):\n",
    "    constraining_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "    return constraining_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Constraining words for whole report\n",
    "\n",
    "def constrain_word_whole(mdaText,qqdmrText,rfText):\n",
    "    wholeDoc = mdaText + qqdmrText + rfText\n",
    "    constrainWordnumWhole =0\n",
    "    rawToken = tokenizer(wholeDoc)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnumWhole +=1\n",
    "    sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "    return sumConstrainScoreWhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "inputDirectory = r'C:\\Users\\YASHA\\new'\n",
    "masterFile = r'C:\\Users\\YASHA\\cik_list1.csv'\n",
    "dataList = rawdata_extract(inputDirectory,masterFile)\n",
    "print(dataList)\n",
    "df = pd.DataFrame(dataList)\n",
    "print(df)\n",
    "df['mda_positive_score'] = df.mda_extract.apply(positive_score)\n",
    "df['mda_negative_score'] = df.mda_extract.apply(negative_word)\n",
    "df['mda_polarity_score'] = np.vectorize(polarity_score)(df['mda_positive_score'],df['mda_negative_score'])\n",
    "df['mda_average_sentence_length'] = df.mda_extract.apply(average_sentence_length)\n",
    "df['mda_percentage_of_complex_words'] = df.mda_extract.apply(percentage_complex_word)\n",
    "df['mda_fog_index'] = np.vectorize(fog_index)(df['mda_average_sentence_length'],df['mda_percentage_of_complex_words'])\n",
    "df['mda_complex_word_count']= df.mda_extract.apply(complex_word_count)\n",
    "df['mda_word_count'] = df.mda_extract.apply(total_word_count)\n",
    "df['mda_uncertainty_score']=df.mda_extract.apply(uncertainty_score)\n",
    "df['mda_constraining_score'] = df.mda_extract.apply(constraining_score)\n",
    "df['mda_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['mda_positive_score'],df['mda_word_count'])\n",
    "df['mda_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['mda_negative_score'],df['mda_word_count'])\n",
    "df['mda_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['mda_uncertainty_score'],df['mda_word_count'])\n",
    "df['mda_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['mda_constraining_score'],df['mda_word_count'])\n",
    "\n",
    "df['qqdmr_positive_score'] = df.qqd_extract.apply(positive_score)\n",
    "df['qqdmr_negative_score'] = df.qqd_extract.apply(negative_word)\n",
    "df['qqdmr_polarity_score'] = np.vectorize(polarity_score)(df['qqdmr_positive_score'],df['qqdmr_negative_score'])\n",
    "df['qqdmr_average_sentence_length'] = df.qqd_extract.apply(average_sentence_length)\n",
    "df['qqdmr_percentage_of_complex_words'] = df.qqd_extract.apply(percentage_complex_word)\n",
    "df['qqdmr_fog_index'] = np.vectorize(fog_index)(df['qqdmr_average_sentence_length'],df['qqdmr_percentage_of_complex_words'])\n",
    "df['qqdmr_complex_word_count']= df.qqd_extract.apply(complex_word_count)\n",
    "df['qqdmr_word_count'] = df.qqd_extract.apply(total_word_count)\n",
    "df['qqdmr_uncertainty_score']=df.qqd_extract.apply(uncertainty_score)\n",
    "df['qqdmr_constraining_score'] = df.qqd_extract.apply(constraining_score)\n",
    "df['qqdmr_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['qqdmr_positive_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['qqdmr_negative_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['qqdmr_uncertainty_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['qqdmr_constraining_score'],df['qqdmr_word_count'])\n",
    "\n",
    "df['rf_positive_score'] = df.riskfactor_extract.apply(positive_score)\n",
    "df['rf_negative_score'] = df.riskfactor_extract.apply(negative_word)\n",
    "df['rf_polarity_score'] = np.vectorize(polarity_score)(df['rf_positive_score'],df['rf_negative_score'])\n",
    "df['rf_average_sentence_length'] = df.riskfactor_extract.apply(average_sentence_length)\n",
    "df['rf_percentage_of_complex_words'] = df.riskfactor_extract.apply(percentage_complex_word)\n",
    "df['rf_fog_index'] = np.vectorize(fog_index)(df['rf_average_sentence_length'],df['rf_percentage_of_complex_words'])\n",
    "df['rf_complex_word_count']= df.riskfactor_extract.apply(complex_word_count)\n",
    "df['rf_word_count'] = df.riskfactor_extract.apply(total_word_count)\n",
    "df['rf_uncertainty_score']=df.riskfactor_extract.apply(uncertainty_score)\n",
    "df['rf_constraining_score'] = df.riskfactor_extract.apply(constraining_score)\n",
    "df['rf_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['rf_positive_score'],df['rf_word_count'])\n",
    "df['rf_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['rf_negative_score'],df['rf_word_count'])\n",
    "df['rf_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['rf_uncertainty_score'],df['rf_word_count'])\n",
    "df['rf_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['rf_constraining_score'],df['rf_word_count'])\n",
    "\n",
    "df['constraining_words_whole_report'] = np.vectorize(constrain_word_whole)(df['mda_extract'],df['qqd_extract'],df['riskfactor_extract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTextCol = ['mda_extract','qqd_extract','riskfactor_extract']\n",
    "finalOutput = df.drop(inputTextCol,1)\n",
    "\n",
    "finalOutput.head(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalOutput.to_csv('textAnalysisOutput.csv', sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
